               INSTRUCTIONS FOR EYETRACKER DATA ANALYSIS

                       (DATA FROM PCEXPT SYSTEM)
                                   
                       7/24/88 (revised 2/10/90)
                                   
                                  CEC

I. OVERVIEW

There are, at the present time, three main programs for analyzing
eyetracking data generated by the PCEXPT system for sentence-reading
experiments:

   EYEWASH. This program takes the raw data generated by the
experiment-running program and creates a new, cleaned-up data file for
each subject. The cleaning-up process consists in (a) manually
correcting erroneous vertical position (y value) readings and (b)
automatically correcting deviantly short or long fixation durations
(with the opportunity to make further corrections manually).

   EYEDRY. This program takes the cleaned-up output of EYEWASH (or a
raw data file, if you trust it), and performs several analyses on each
of the regions you define in your sentences. The analyses include first
fixation durations and frequencies, first pass reading time durations,
total reading time durations, the probabilities of regressions out of,
and into, each specified region, and the probability of fixating in
each region.

   EYEDIST. This program calculates the frequency distributions of
various measures of eyetracking behavior.

II. EYEWASH

   A. To run EYEWASH, you must first create a xxx.SEN and a xxx.POS
file for each counterbalancing condition in your experiment. You do
this by making a copy of the boilerplate program SENPOS.TEM, renaming
it, adding the core of your experiment-running program where the
boilerplate indicates, and then running it once for each
counterbalancing condition. These .SEN and .POS files contain the
actual words, and the starting positions of each word (taken to be the
space in front of the word), that subjects read in the appropriate
counterbalancing conditions. Copy SENPOS.TEM from C:\DATA onto your
subdirectory, renaming it something reasonable (e.g., exnameSP.C, where
"exname" is the name of your experiment). Insert the main part of your
experiment running program (the part that defines cond_adjust and uses
pickone()) where it says "START OF YOUR MAIN PROGRAM HERE." Compile it
using ETCC (or ETCC206).

   B. Run the resulting program one time for each counterbalancing
condition in the experiment. When it asks for input file, tell it the
name of the sentence file you used in your experiment. When it asks for
data file, just press the ENTER key. When it asks for condition number,
tell it the counterbalancing condition you are working on. When it asks
for sentence and position output file names, give it appropriate names
for each counterbalancing condition (e.g., exnameC#.SEN for condition
#), using the extensions .SEN and .POS.

   C. Run EYEWASH on each subject's raw data file. EYEWASH will create
a new, corrected data file, using the name you provide (use the same
name as the input data file, with an extension like .DA1) and it will
create an intermediate data file, using its own name (xxx.&A1). 

   EYEWASH has two parts. The first part allows you to correct deviant
y values. The second part ("final fix") corrects deviant-length
fixations. The program begins by asking you for the names of the .SEN,
.POS, and data files you will be using. It then requires you to set two
cutoffs that are used in final fix. The first cutoff pools fixations
that are very close together (e.g., within one character), one of which
is below a specified msec. cutoff (e.g., 80 msec). The second cutoff
eliminates fixations that are also short (generally shorter than 80
msec), and moderately close to the nearest other fixation. The program
suggests a cutoff of 40 msec and 3 characters, but these can be
changed. The logic is that the first sort of short fixation is probably
really part of a fixation, while the second is a glitch during a
saccade and should be treated as part of a saccade.

   The program then asks whether you want to do all the initial
screening (y value correction) and then the final fix, or do everything
sentence by sentence. If you think you might want to quit in the
middle, choose the first option. It will save an intermediate file
(with an .&?? extension) that is the result of initial screening, and
if you use this intermediate file as input later, you can skip the
initial screening.

   The program then proceeds to the initial screening, asking you for x
and y offset. These are the values (positive integers) needed to move
the display the subject saw to the upper left corner of the TARGA
screen (0,0). X is generally 0; y depends on your program, but will be
6 if you displayed the first line of text 6 lines from the top. The
program displays each fixation it thinks may be deviant (because its y
value changed from the previous one) plus several surrounding
fixations, and lets you change the y value if you like. You can move
backward in a sentence to check earlier fixations by typing the earlier
fixation number. You can terminate a sentence (deleting all fixations
after the current one) by telling the program to go to fixation 999.
You can move backward to an earlier sentence by answering "n" when
asked if you want to go on to the next sentence, and then telling the
program the number of of the sentence you want to go back to (the
sequence number in the data file). You can change a block of y values
to a common value by typing -1 when it asks you which y value to change
and then answering the questions about defining the block. The output
of this initial screening sets the y values for the first line of the
display to 0.

   After initial screening, final fix happens. The program
automatically corrects fixations that pass the criteria you set. It
then allows you to manually change other fixations for each sentence.
In general, you won't want to make any manual changes; this step just
lets you screen what the program did. If you have a bad trial, final
fix lets you throw it out, by typing 999 when prompted. (This adds 1000
to the condition number.) The program writes a pre- and post-adjusted
list of fixations for the sentence on the printer connected to the
computer (or a specified listing file, if you prefer), and it writes a
file of corrected data, suitable for input to EYEDRY.

III. EYEDRY

   A. Overview. Eyedry computes first fixation duration, first pass
duration, total reading time, the percentage of regressions out of each
region, the percentage of regressions into each region, the percentage
of times each region was fixated on the first pass, and the second pass
duration, for each specified region of each input sentence in an
experiment, as well as the concentrated looks at a single region, which
used to be called EYEPOKE and EYEGAZE (see below). It also reports the
counts of the numbers of subjects who contributed observations to each
region. A region can be a single word, or it can be a series of words,
or it can be a portion of a word. Imagine that you have reading times
for sentences like (1), in which regions are separated by '^':

   (1a). The big bully^ always thought^ he knew^ girls^ bite back^
sometimes.
   (1b). Friends and neighbors^ claim^ that^ my time^ has^ finally^
come.

You may want to compare the reading times for the "disambiguating"
regions of these sentences, "bite back" in (1a) and "has" in (1b). This
region is the 5th region in each sentence (regions are separated by ^).
You would like to get the mean reading times for (a) this critical
region, (b) the region right after it, (c) the region right before it,
and (d) the disambiguating complementizer "that" in the second
sentence. That's what EYEDRY does for you: It will write out disk files
that contain this information for each condition of an experiment on
both a subject-by-subject basis and an item-by-item basis.

   B. First, prepare a .CNT file. Copy the boilerplate WORDPOS.TEM from
C:\DATA onto your subdirectory, rename it (e.g., expnmWP.C), stick the
core of your experiment-running program where it says "YOUR PROGRAM
STARTS HERE," compile it with ETCC or ETCC206. Then run it once. (You
can make separate .CNT files for separate subexperiments by making
copies of the relevant parts of the overall .CNT file, or you could run
the expnmWP program several times, or you can leave the overall .CNT
file intact and have EYEDRY select the appropriate parts from it.) The
program expnmWP runs basically like your experiment-running program
ran, except (a) you should tell it 1 for subject condition and
counterbalancing condition, and (b) you should indicate no data file.
When it asks for the position count output file, tell it the name you
want to give your .CNT file (include the extension). If you like, the
program will give you a hard copy of the sentences as seen by your
subjects, broken up into regions.

   If you want your regions to be single words, just press the SPACE
bar when the program asks for delimiters (thereby telling it that the
delimiter is ' '). Otherwise, prepare a file that is a modification of
your experiment .SEN file. You should indicate the end of each region
you want to analyze in this file, by inserting some convenient
character at the end of each region of each sentence (like the ^ in
(1), above), and use this as the input file. (NOTE CAREFULLY: The
delimiter comes right after the last character in a region, and before
the first character in the next region - which in general will be the
space before the first word in that region.) Tell the program the
delimiting character you used. Make sure that the regions you want to
compare across sentences have the same ordinal positions in each
sentence (in the example above, the critical region had position 5).

   C. Prepare a file of data file names. Call it (by default) DATA.LST.
One data file name (the output of EYEWASH) should be on each line. For
example:

   T1.DA1
   T2.DA1
   T3.DA1

   D. Examine a data file to see what column contains the condition
number, the item number, the "number of observations" number (number of
fixations in the sentence), and the start of the vector of reading
times. (Typically, these will be columns 2, 3, 8 and 9.)

   D.Run EYEDRY. Type a convenient label, which will be printed on the
printed output as an identifier. The program types out the date
automatically. If you have run EYEDRY before in an experiment, you can
tell in to load a "control" file (generally, .CON or .CRN) in which you
have saved your answers to a bunch of questions. Otherwise, you will
have to answer some questions (after which you will have a chance to
save your answers in a disk file). The first question asks you about
debug level. Type 0. Tell it no, there are not questions after every
item (unless there are, and they are on a different line than the item;
and even here, you can generally tell it no anyway). Answer the rest of
the questions. Tell it the smallest and the largest sentence numbers
and the smallest and largest condition numbers you plan to analyze.
NOTE: This lets you analyze a big experiment with multiple
subexperiments in it component by component. You can have separate .CNT
files for each subexperiment, or you can have just one .CNT file. Tell
it the names of your file of data file names (DATA.LST) and the .CNT
file (e.g., expnm.CNT) when it asks. Tell EYEDRY which analysis you
want it to do; it will loop through the possible analyses until you
tell it you are done. For each analysis, give EYEDRY the names of the
files you want it to output data to. These will be inputs to the
subjects and the items ANOVAS. You will generally not want to have it
write a file of subjects x items combinations, unless each subject is
tested in all conditions on each item.

   E. EYEDRY allows you to have an exceptions file (which has entries
like:
        item#     +-changetocondition
        item#     +-changetocondition
This lets you change the condition number of specified items (e.g., to
correct errors in the original sentence file, or -- by specifying +99
as changetocondition -- to delete an item entirely). EYEDRY also allows
you to discard trials when a question was answered incorrectly, if you
wish.

   The output of EYEDRY is suitable for input directly to COLANOV, a
very simple-minded ANOVA program (and by adding group numbers, can be
used in SYSTAT, ECSTATIC, etc.). As well as containing the data you
asked for, most data files also contain the number of observations that
contributed to each datum. The data are formated as "(10x,n(f6.0,4x))"
for for n regions of an item. They are arranged subject-by-subject (or
item-by-item), subjects (items) moving most slowly, and within each
subject (item), they are arranged by condition, lowest to highest, one
condition per line.

   I promised you information about the EYEPOKE and EYEGAZE routines of
EYEDRY. OK. EYEPOKE computes the duration of 3 fixations before and 3
fixations after the first fixation in a single specified region, or the
mean of the signed values of the distances of each of these fixations
from the first fixation specified in the region. The first distance can
be interpreted as the mean absolute value of the length of the first
saccade, either regressive or progressive, after the fixation; but the
other distances can only be interpreted as distances, since each
fixation position results from a mixture of regressive and progressive
saccades. EYEGAZE computes the first pass duration on a single
specified region (which, if it is a single word, results in gaze
duration for that word).
   EYEPOKE's and EYEGAZE's big trick is that, if they don't find a
fixation in the specificed region, they move the left boundary of a
region left, character by character, until they find a fixation or
until they have moved the region as far left as you permitted it to;
then they try once again, moving the right boundary one character
right. Then they give up. (They never move a boundary around a line
break). Thus, EYEGAZE and EYEPOKE minimize the problem of missing data
(under the assumption that information from a region can be acquired
from just outside that region.

IV. EYEDIST

   EYEDIST computes distributions of fixation durations for a single
specified region. It is used just like EYEDRY (although at the present
time it doesn't save the answers to questions), and it loops through
several possible analyses: the distribution of durations of the first
fixations in the specified region, the distribution of the sums of all
first-pass fixations in the region, the distribution of the sums of all
fixations in the region, and the distribution of the durations of all
individual fixations in the region. It computes, and prints out, the
percentages of the entire distribution that fall in each of 20 equally-
spaced "bins," separately for each condition, and it writes a data file
containing each distribution for each condition, suitable for input to
a graphing program. The number of bins can be changed by re#DEFINING
NBINS in the program EYEDIST.C and recompiling it by typing TCC -ml
eyedist


COMMENTS ON THE PROGRAMS AND SUGGESTIONS FOR IMPROVEMENT ARE GLADLY
RECEIVED. COMMENTS ON THIS WRITE UP SHOULD TAKE THE FORM OF WRITTEN
MODIFICATIONS. 

--CEC



INSTRUCTIONS IN USING MRT

0. Overview. MRT is a program that takes individual reaction times and
responses and prints out the average RT in each condition of an
experiment, together with the proportion of correct responses in each
condition. It also writes disk files containing the mean RTs and prop
correct values for each condition, both subject by subject and item by
item. These disk files are appropriate for input to an ANOVA. 

   The input to MRT is assumed to be disk files of data, collected
under the PCEXPT or EXPT86 system. There is one such disk file for each
subject. The files contain ASCII integers, arranged in columns of
numbers. (Width of columns doesn't matter.) One row contains the data
for one trial (combining, perhaps, both the data from an item and the
data from a question about that item). One column must contain
condition number; another must contain item number; a third must
contain RT; and a fourth must contain something perversely referred to
as "subcondition," which actually is "response" (generally, correct =
1, error = 2, but this can vary).

   The output can be thought of as four matrices, two for mean RTs and
two for proportion correct scores. The matrices have subect or item as
the rows, and "subcondition" (=response) as the columns. The first
column of each matrix is special. It is the average RT or prop correct
over a specified range of the other columns (responses, subconditions).
For example:

       SUBJECT    SPECIAL          SUBCONDITIONS
                  (MEAN)             1       2
         1         500              500     650
         2         400              400     300
                           ...

Here, the special (mean) column is the mean for subcondition 1 (which
is in general the correct response subcondition). These four matrices
are actually written out as eight: Four consist strictly of the special
(mean) values, and four of the subcondition values alone. 

To go ahead and use MRT, here's what to do:

I. Plan ahead. Decide how many subexperiments your experiment contains.
You will want to run MRT once on each subexperiment, to write different
output files for each subexperiment. Then, for each subexperiment,
decide on the following (write them down, if you like):

   a. the smallest condition number in the subexperiment (i.e., the
   smallest condition number you want to pay take responses from)
   b. the largest condition number
   c. the smallest and the largest item numbers, and how many distinct
   items you want the data stored under (this can be the total number
   of items, or something less if different items have different
   condition numbers and you want to save space)
   d. how many different subclassifications (possible response values)
   there are
   e. how many of these are to be included in the averaged data.
   f. how many of these subclassifications are "correct" for the
   purposes of calculating the proportion correct scores. 
   g. what the actual values of the subclassifications are (i.e., what
   numbers are in the individual subject data files). If your data
   files have "1" for correct response and "2" for error (the normal
   convention), 1 and 2 are the values of the subclassifications. If
   you have n different subclassification values, m of which are
   associated with responses that should be included in the average
   data, you will want to name the m to-be-included values first,
   before any of the not-included ones (and similarly, if k values
   correspond to correct responses, you will want to mention these k
   first before any incorrect response values).
   h. Examine one of your data files, and decide which column your
   condition number, item number, RT, and subclassification are in. An
   example:
       order  cond   item   RT   resp   RT resp
        90     2       1   400    1     0   0
        47     3       2   500    2     0   0
   Condition number is in column 2, item 3, RT 4, and subclassification
   5. The subclassification values will be 1 and 2; you will probably
   want to sum over the first of these, and count it as the correct
   response.
   i. decide on the maximum RT you will allow, both in absolute msec
   terms, and in standard deviations from the mean (subject by subject)
   terms. 
   j. decide whether you want to discard a too-long time, or replace it
   with the cutoff value
   k. decide on the minimum absolute RT you will accept
   l. decide on the name of the data file for RT means, subject by
   subject, averaged over the specified subconditions. This file will
   contain the "special column" of the data matrix referred to above.
   (You can skip this step, and MRT will just assign a default name:
   SUBFILE.DAT.) 
   m. decide how many data values you want on each line of this data
   file (generally, the number of conditions in the subexperiment, but
   if that is inconveniently large, a submultiple of that number is
   OK).
   n. decide on the name of the data file for RT values,
   subclassification by subclassification. This will be the body of the
   matrix referred to above, less the special column.
   o. do l, m, and n, for the item-by-item RT data files, and then do
   the whole mess again for the proportion correct files.
   p. an addition in December 1989: You can classify the responses
   under the condition number of the trial BEFORE the response was
   made, if you like. This is useful if you are using MRT to score
   answers to questions presented immediately after the trial of
   interest is over, and you want to classify these answers according
   to the condition number of the previous trial. Just be prepared to
   answer 'y' when the program asks you if you want to classify
   responses under the condition of the previous trial, and then to
   provide the MINIMUM and MAXIMUM condition numbers for this previous
   trial (as distinct from the MINIMUM and MAXIMUM condition numbers
   for the question trial itself, which you provided earlier).
   q. and another December addition: Decide whether you want to analyze
   means or medians and answer the question when it is asked

II. Now that your planning is done, prepare a file (call it DATA.LST as
a default) that contains the names of all the individual subject data
files, one per line. For example:
   S1.dat
   S2.dat
   S3.dat....    if you named your data file S1.DAT, etc.

III. Run MRT. Go to the disk and directory containing your data and the
DATA.LST file, and type MRT. Type 0 when it asks you for debug level.
Press "ENTER" when it asks you for the file that contains the analysis
information (assuming that this is the first run in an experiment).

IV. Answer all the questions. You will have the answers available, if
you planned ahead as described above. When you have answered all the
questions, your answers will be displayed on the screen, and you will
be given a chance to correct any you want to correct.

V. When you have made all necessary corrections, press "ENTER." Then
type in the name of the file you want to save your answer as, when it
asks. ALWAYS USE THE EXTENSION .CRN. For example, TENSE.CRN. NEVER
NEVER NEVER type DATA.LST at this point. You'll wipe out the file you
prepared in step II above. (The point of all this is that, when you run
MRT next time, you can give it the name of this file when it asks you
for the name of the            file that contains the analysis
information, and skip having to answer the questions again.)

VI. When it asks you for the name of the file containing data file
names, type DATA.LST (or whatever you called it).

VII. When it asks you, do you want individual subject means etc. typed
out, tell it yes (unless you have done the analysis before, and want to
speed things up).

VIII. When it asks you, is there an exceptions file, tell it no, unless
there is one. There will be if you made mistakes in your experiment
program, and want to change the condition numbers for certain items. An
exception file consists of a list of pairs of item numbers and the
amount you want to increase or decrease the condition numbers for that
item by, one pair per line, eg:
   17  4
   22  -4
   47  99

IX. When it asks you, tell it yes, you want the item by item data files
written (if you will be doing item ANOVAs, that is), but unless you
want to really poke around in the data, tell it no, you don't want the
item by item data typed out.

X. Check the RT data files for missing values (0) before running
ANOVAS, and correct these missing values in your favorite way (e.g., by
using the program FOO). 



                    INSTRUCTIONS FOR USING SEGMENT,
              SELFPACE.TEM, COUNT.TEM, MSPERCH.C, CORR.C
                                 11/89

0. Overview. SEGMENT is a program that takes vectors of self-paced
reading times (or pigtracking deviation scores) and computes the
averages of specified portions of these vectors. Imagine that you have
phrase by phrase reading times for sentences like:
   The big bully^always thought^he knew^girls^bite back^ sometimes.^
   Friends^and neighbors^claim^that^my time^has^finally^come.^
You may want to compare the reading times for the "disambiguating"
regions of these sentences, "bite back" in the first sentence and "has"
in the second. This region is the 5th presentation segment in the first
sentence (phrases are separated by ^), but the 6th in the second
sentence. You would like to get the mean reading times for (a) this
critical analysis segment (b) the segment right after it (c) the
segment right before it and (d) the disambiguating complementizer
"that" in the second sentence. That's what SEGMENT does for you: It
will write out disk files that contain this information for each
condition of an experiment on both a subject by subject basis and an
item by item basis.

1. .CNT (or .SEG) file. The first thing to do is to prepare a .CNT (or
.SEG) file. You do this using the .C template SELFPACE.TEM. First, make
a copy of your sentence file, the .SEN file. (It's a good idea to
rename it with a "d" in the name, indicating a "delimited" file.)
Choose some character that is otherwise unused in the file, such as a
~, to mark the analysis segments. Identify each of the presentation
segments that should BEGIN a new analysis segment (except don't bother
with the very first analysis segment; the program assumes the sentence
begins at the beginning). Place the analysis segment character
immediately before each such analysis segment (i.e., AFTER the
presentation segment character that ends an analysis segment, the '^'
in the example above; note, don't put an analysis segment character
after the end of the sentence). You would wind up with a file looking
like this:

The big bully^always thought^~he knew^~~girls^~bite back^~sometimes.^
The big bully^always thot^~he knew^~that^~girls^~bite back^~sometimes.^
Friends^and neighbors^~claim^~~my time^~has^~finally^~come.^
Friends^and neighbors^~claim^~that^~my time^~has^~finally^~come.^

In this file, the disambiguating segment "bite back" or "has" is the
fifth analysis segment (even though it was sometimes the fifth and
sometimes the sixth presentation segment). Note that no analysis
character has to come after the last presentation segment. Remember,
the analysis characters PRECEDE each analysis segment. Remember,
further, that you may have had a presentation segment character at the
very start of the sentence. This will present the preview characters
before the sentence begins, and the very first reading time will in
fact be the time the subject looked at the preview characters. If you
want to keep this preview time separate from reading time, you will
have to put an analysis segment character right after this presentation
segment character (right before the first word of the sentence). For
example:
^~The big bully^always thought^~he knew^~~girls^~bite back^~
sometimes.^

Remember, finally, that if you have done a word by word self-paced
experiment, you will need to have an analysis character right before
the start of each analysis region, that is, right AFTER the space that
follows the end of the previous analysis region. You cannot have an
extra space at the end of a line, before the newline (or CR/LF)
character. Get rid of these. They don't hurt in the running program,
and they shouldn't hurt in general in the analysis program, but there
are some circumstances where they can mess things up. Also, when an
analysis segment begins a new line, you have to have the analysis
segment character at the start of this line, NOT at the end of the
previous line.

Once you have this xxxD.SEN file made up, you should copy the
SELFPACE.TEM file into your subdirectory, renaming it something
reasonable (xxxSP.C). Then copy the body of your experiment-running
program (the stuff that selects the proper versions of the sentences)
into the clearly-indicated place in the program (being sure not to
include anything like CTRIAL++ or *storfn(), which increases trial
number). Then compile the xxxSP.C program using ETCC (or ETCC206; it
doesn't much matter) and run it, answering the questions it asks about
number of counterbalancing conditions, maximum condition and item
number, etc. The way things look now, it will be fine to include all
subexperiments (all experimental sentences) in one file. The outcome of
all this is a file with one line for each condition of each sentence,
containing item number, condition number, number of presentation
regions, and the start of each successive analysis region (in
presentation region units). It assumes that the first analysis segment
starts with presentation segment 1 (which would be any preview
material), but doesn't explicitly indicate this. Each line ends with a
0.

2. Prepare a file of data file names. Call it (by default) DATA.LST.
One data file name should be on each line. For example:

   T1.DAT
   T2.DAT
   T3.DAT

Remember, don't put an extra carriage return at the end of the file.
Doing that will count the last subject twice.

4. Examine a data file to see what column contains the condition
number, the item number, the "number of observations" number (number of
segments in the sentence), and the start of the vector of reading
times. Typically, this will be columns 2, 3, 8 and 9. Also determine
the smallest and largest condition and item numbers in the experiment
you are analyzing. Often it is desirable to analyze an experiment in
parts, analyzing each subexperiment separately.

5.Run SEGMENT, once for each separately-analyzed subexperiment. If it
is the first time you are running it in your experiment, just type
ENTER (CR) when it asks you for a .CRN file name, and then answer the
questions. Type 0 as debug level. Tell it no, there are not questions
after every item (unless for some unfathomable reason, you wrote your
experiment-running program to put the question responses on the same
line as the reading times). Answer the rest of the questions, in the
obvious manner (he says). When you are done answering the questions,
you will be able to change any of the answers, and then save the
answers away as a .CRN file for use the next time you run SEGMENT. Then
indicate the file of data file names (DATA.LST) and the file of
analysis segment starts (the .CNT or .SEG file). Your reward for all
this is a typed table (or if you didn't want hard copy output, a disk
file) of the mean reading times in each analysis segment, each
position, and files of the subject-by-subject, item-by-item, and
subject-by-item data breakdowns (if you requested them). These output
files should be checked for missing data, and then can be submitted to
COLANOV (or with a little massaging, SYSTAT or ECSTATIC).

6. Generally, it is wise to do all this on the raw data and on data
adjusted for the length of the segments (eg, using linear regression,
or msec/char adjustment). To do the msec/char, first find the COUNT.TEM
program, copy it into your subdirectory as xxxCNT.C, add the guts of
your program like you did to SELFPACE.TEM, and compile it with ETCC.
Then run it on the .SEN file you used in the experiment, ONCE FOR EACH
COUNTERBALANCING CONDITION. Tell it the sentences and conditions you
want to analyze, and it will ignore the others. It will create a file
of the numbers of characters in each presentation segment for each
counterbalancing condition. Save these files away with different names,
eg., xxx1.CHR, xxx2.CHR, xxx3.CHR. NOTE: Unlike the other programs in
this suite, you assign these names when the program asks you for data
file name.

Then, make up a variant of DATA.LST (call it something like
DATACHR.LST) that has, on each line, a subject's data file name
followed by the .CHR file for that subject's counterbalancing
condition. Then, run MSPERCH, telling it DATACHR.LST when it asks for
the file of data and count files. This will create a new data file for
each subject, with the same name as the old data file but with a C
appended at the start. You can run SEGMENT on these. But remember,
since these are msec/char values, you will have to change the upper and
lower cutoffs.

There is a similar program called CORR that calculates the linear
regression of reading time on length and write files of the deviations
from the predicted times. It works OK, but not quite like MSPERCH -
yet. When I get it updated, I'll add these instructions.

7. If you don't like reading these instructions, don't complain, just
figure out how to do the analyses and then re-write the instructions.